{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from setproctitle import setproctitle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from rank_bm25 import BM25L\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "import argparse"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T05:51:15.085002200Z",
     "start_time": "2024-03-29T05:51:04.355454800Z"
    }
   },
   "id": "5d8a789517867888"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-29T05:51:15.157636300Z",
     "start_time": "2024-03-29T05:51:15.084004500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "''' 아무 내용이 없는 줄은 제거합니다. '''\n",
    "def get_rid_of_empty(c):\n",
    "    ret = []\n",
    "    splitted = c.split('\\n')\n",
    "    for s in splitted:\n",
    "        if len(s.strip()) > 0:\n",
    "            ret.append(s)\n",
    "    return '\\n'.join(ret)\n",
    "\n",
    "\n",
    "def clean_data(script, data_type=\"dir\"):\n",
    "    preproc_lines = []\n",
    "\n",
    "    if data_type == \"dir\":\n",
    "        with open(script, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "    else:\n",
    "        lines = script.split('\\n')\n",
    "\n",
    "    block_comment = False  # 블록 주석이 시작되었는지 여부를 추적\n",
    "    for line in lines:\n",
    "        line = line.rstrip()  # 오른쪽 공백 제거\n",
    "\n",
    "        # 블록 주석 시작 처리\n",
    "        if '/*' in line:\n",
    "            block_comment = True\n",
    "            line = line.split('/*', 1)[0]  # '/*' 이전 내용만 유지\n",
    "\n",
    "        # 블록 주석 내용은 모두 무시\n",
    "        if block_comment:\n",
    "            if '*/' in line:  # 블록 주석 종료 확인\n",
    "                line = line.split('*/', 1)[1]  # '*/' 이후 내용만 유지\n",
    "                block_comment = False\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # 한 줄 주석 처리\n",
    "        if '//' in line:\n",
    "            line = line.split('//', 1)[0]\n",
    "\n",
    "        # 불필요한 공백과 탭 정리\n",
    "        line = line.replace('\\n', '')\n",
    "        line = line.replace('    ', '\\t')  # 4개의 공백을 탭으로 변환\n",
    "\n",
    "        if line == '':  # 빈 라인 무시\n",
    "            continue\n",
    "\n",
    "        preproc_lines.append(line)\n",
    "\n",
    "    preprocessed_script = '\\n'.join(preproc_lines)\n",
    "\n",
    "    return preprocessed_script\n",
    "\n",
    "\n",
    "\n",
    "''' positive, negative 페어 생성 함수 '''\n",
    "def get_pairs(input_df, tokenizer):\n",
    "    codes = input_df['code'].to_list()\n",
    "    problems = input_df['problem_num'].unique().tolist()\n",
    "    problems.sort()\n",
    "\n",
    "    tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "    bm25 = BM25L(tokenized_corpus)\n",
    "\n",
    "    total_positive_pairs = []\n",
    "    total_negative_pairs = []\n",
    "\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes = input_df[input_df['problem_num'] == problem]['code']\n",
    "        positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "        solution_codes_indices = solution_codes.index.to_list()\n",
    "        negative_pairs = []\n",
    "\n",
    "        first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "        negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "        negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "        ranking_idx = 0\n",
    "\n",
    "        for solution_code in solution_codes:\n",
    "            negative_solutions = []\n",
    "            while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "                high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "                if high_score_idx not in solution_codes_indices:\n",
    "                    negative_solutions.append(input_df['code'].iloc[high_score_idx])\n",
    "                ranking_idx += 1\n",
    "\n",
    "            for negative_solution in negative_solutions:\n",
    "                negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "        total_positive_pairs.extend(positive_pairs)\n",
    "        total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "    pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "    pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "    neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "    neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "    pos_label = [1]*len(pos_code1)\n",
    "    neg_label = [0]*len(neg_code1)\n",
    "\n",
    "    pos_code1.extend(neg_code1)\n",
    "    total_code1 = pos_code1\n",
    "    pos_code2.extend(neg_code2)\n",
    "    total_code2 = pos_code2\n",
    "    pos_label.extend(neg_label)\n",
    "    total_label = pos_label\n",
    "    pair_data = pd.DataFrame(data={\n",
    "        'code1':total_code1,\n",
    "        'code2':total_code2,\n",
    "        'similar':total_label\n",
    "    })\n",
    "    pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "    return pair_data\n",
    "\n",
    "def data_preprocess(args):\n",
    "    \"\"\" Data preprocess\n",
    "    train, valid, test에 대한 전처리를 수행하고, 이 과정에서 결과적으로 아래의 파일명이 생성됩니다.\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    dacon_train_data = pd.read_csv(\"./data/\" + \"new_dataset_0607/graph_dacon_train_bm25L.csv\")\n",
    "    dacon_valid_data = pd.read_csv(\"./data/\" + \"new_dataset_0607/graph_dacon_valid_bm25L.csv\")\n",
    "    codenet_train_data = pd.read_csv(\"./data/\" + \"new_dataset_0607/graph_codenet_train_bm25L.csv\")\n",
    "    codenet_valid_data = pd.read_csv(\"./data/\" + \"new_dataset_0607/graph_codenet_valid_bm25L.csv\")\n",
    "    test_data = pd.read_csv(\"./data/new_dataset_0604/processed_test.csv\")\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이콘이 제공해준 학습 코드 데이터 데이터프레임 만들기\n",
    "    code_folder = \"../data/train_code\"  # 데이콘이 제공해준 학습 데이터 파일의 경로\n",
    "    problem_folders = os.listdir(code_folder)\n",
    "    preproc_scripts = []\n",
    "    problem_nums = []\n",
    "\n",
    "    for problem_folder in tqdm(problem_folders):\n",
    "        scripts = os.listdir(os.path.join(code_folder, problem_folder))\n",
    "        problem_num = scripts[0].split('_')[0]\n",
    "        for script in scripts:\n",
    "            script_file = os.path.join(code_folder, problem_folder, script)\n",
    "            preprocessed_script = clean_data(script_file, data_type=\"dir\")\n",
    "            preproc_scripts.append(preprocessed_script)\n",
    "        problem_nums.extend([problem_num] * len(scripts))\n",
    "    train_df = pd.DataFrame(data={'code': preproc_scripts, 'problem_num': problem_nums})\n",
    "\n",
    "    # 데이콘이 제공해준 테스트 코드 데이터 데이터프레임 만들기\n",
    "    test_df = pd.read_csv(\"../data/test.csv\")\n",
    "    code1 = test_df['code1'].values\n",
    "    code2 = test_df['code2'].values\n",
    "    processed_code1 = []\n",
    "    processed_code2 = []\n",
    "    for i in tqdm(range(len(code1))):\n",
    "        processed_c1 = clean_data(code1[i], data_type=\"file\")\n",
    "        processed_c2 = clean_data(code2[i], data_type=\"file\")\n",
    "        processed_code1.append(processed_c1)\n",
    "        processed_code2.append(processed_c2)\n",
    "    processed_test = pd.DataFrame(list(zip(processed_code1, processed_code2)), columns=[\"code1\", \"code2\"])\n",
    "\n",
    "    # IBM의 CodeNet으로 추가 코드 학습/검증 데이터 데이터프레임 만들기\n",
    "    code_folder = \"Project_CodeNet_Python800\"  # CodeNet 데이터 경로\n",
    "    problem_folders = os.listdir(code_folder)\n",
    "    preproc_scripts = []\n",
    "    problem_nums = []\n",
    "    for problem_folder in tqdm(problem_folders):\n",
    "        scripts = os.listdir(os.path.join(code_folder, problem_folder))\n",
    "        problem_num = int(problem_folder.split('p')[1])\n",
    "        problem_num = 'problem' + str(problem_num)\n",
    "        for script in scripts:\n",
    "            script_file = os.path.join(code_folder, problem_folder, script)\n",
    "            preprocessed_script = clean_data(script_file)\n",
    "            preproc_scripts.append(preprocessed_script)\n",
    "        problem_nums.extend([problem_num] * len(scripts))\n",
    "    codenet_df = pd.DataFrame(data={'code': preproc_scripts, 'problem_num': problem_nums})\n",
    "\n",
    "\n",
    "    \"\"\" 추가 codenet 데이터에서 테스트셋과 겹치는 데이터가 있다는걸 관찰하였고, 이를 위해 3단계에 걸쳐 필터링 작업을 진행합니다.\n",
    "\n",
    "    [1차 필터링] : codenet_df에서 test_df의 데이터와 겹치는 녀석들을 set (hash table) 으로 대부분 필터링해줍니다.\n",
    "    [2차 필터링] : 1차 필터링 과정에서 trailing space 등의 이유로 set 방법으로 완전하게 걸러지지 않은 데이터를 걸러주는 것이 목적입니다. 이를 위해 코드 문자열에 존재하는 newline들을 전부 이어붙이고, 앞 뒤로 존재하는 공백 및 newline을 제거합니다. 이렇게 전처리된 test code 와 codenet code 문자열들을 각각 set에 넣어 intersection 함으로써 한번 더 걸러주는 작업을 수행합니다.\n",
    "    [3차 필터링] : 2차 필터링 과정 이후 test set에 포함된 데이터가 거의 다 제거되었습니다. 그러나 완벽히 제거하기 위해 exhaustive search를 통해 최종적으로 남아있는 test셋의 흔적들을 제거합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # [1차 필터링]\n",
    "    dacon_codes = np.concatenate([train_df['code'].values,\n",
    "                                  test_df['code1'].values,\n",
    "                                  test_df['code2'].values]\n",
    "                                 )\n",
    "    dacon_codes_set = set()\n",
    "\n",
    "    for i in tqdm(range(len(dacon_codes))):\n",
    "        dacon_codes_set.add(dacon_codes[i])\n",
    "    usable_codes = []\n",
    "    usable_problem_nums = []\n",
    "    codenet_codes = codenet_df['code'].values\n",
    "    problem_nums = codenet_df['problem_num'].values\n",
    "\n",
    "    for i in tqdm(range(len(codenet_codes))):\n",
    "        if codenet_codes[i] not in dacon_codes_set:\n",
    "            usable_codes.append(codenet_codes[i])\n",
    "            usable_problem_nums.append(problem_nums[i])\n",
    "    filtered_codenet_df = pd.DataFrame(data={'code': usable_codes,\n",
    "                                             'problem_num': usable_problem_nums})\n",
    "\n",
    "    # 리소스 문제로, 완성된 filtered_codenet_df 중 50%의 데이터만을 이용해서 학습에 사용합니다.\n",
    "    filtered_codenet_df = filtered_codenet_df.sample(frac=0.5, random_state=42)\n",
    "\n",
    "\n",
    "    # [2차 필터링]\n",
    "    def simplify(x):\n",
    "        return ''.join(x.split('\\n')).rstrip(' ').strip()\n",
    "\n",
    "    codenet_codes = filtered_codenet_df['code'].values\n",
    "    codenet_problem_nums = filtered_codenet_df['problem_num'].values\n",
    "    test_codes1 = test_df['code1'].values\n",
    "    test_codes2 = test_df['code2'].values\n",
    "\n",
    "    test_codes = np.concatenate([test_codes1, test_codes2])\n",
    "\n",
    "    codenet_set = set()\n",
    "\n",
    "    for i in tqdm(range(len(codenet_codes))):\n",
    "        codenet_set.add(simplify(codenet_codes[i]))\n",
    "    test_set = set()\n",
    "    for i in tqdm(range(len(test_codes))):\n",
    "        test_set.add(simplify(test_codes[i]))\n",
    "    intersection = codenet_set.intersection(test_set)\n",
    "    usable_codenet_filtered, usable_codenet_filtered_problems = [], []\n",
    "    for i in tqdm(range(len(codenet_codes))):\n",
    "        if simplify(codenet_codes[i]) not in intersection:\n",
    "            usable_codenet_filtered.append(codenet_codes[i])\n",
    "            usable_codenet_filtered_problems.append(codenet_problem_nums[i])\n",
    "    filtered_codenet_df = pd.DataFrame(data={'code': usable_codenet_filtered,\n",
    "                                             'problem_num': usable_codenet_filtered_problems})\n",
    "\n",
    "\n",
    "    # [3차 필터링]\n",
    "    codenet_codes = filtered_codenet_df['code'].values\n",
    "    problem_nums = filtered_codenet_df['problem_num'].values\n",
    "    usable_codenet_filtered, usable_codenet_filtered_problems = [], []\n",
    "    for i in tqdm(range(len(codenet_codes)), position=0, leave=True):\n",
    "        usable = True\n",
    "        if codenet_codes[i] in test_set:\n",
    "            continue\n",
    "        else:\n",
    "            for s in test_set:\n",
    "                if len(s) > 0 and len(codenet_codes[i]) > 0 and ((codenet_codes[i] in s) or (s in codenet_codes[i])):\n",
    "                    usable = False\n",
    "                    break\n",
    "        if usable == True:\n",
    "            usable_codenet_filtered.append(codenet_codes[i])\n",
    "            usable_codenet_filtered_problems.append(problem_nums[i])\n",
    "\n",
    "    filtered_codenet_df = pd.DataFrame(data={'code': usable_codenet_filtered,\n",
    "                                             'problem_num': usable_codenet_filtered_problems})\n",
    "\n",
    "\n",
    "    # 데이터 프레임을 만들었으니 이제 train/val split을 진행하고, positive, negative pairs를 생성합니다.\n",
    "    # 청소님의 코드를 참고해서 hard negative pair를 생성하였으며, BM25대신 BM25L을 사용합니다.\n",
    "    # (BM25, BM25L 모두 테스트한 결과 BM25L에서 더 좋은 성능을 보였습니다.)\n",
    "    # tokenizer는 왼쪽부터 truncation을 진행하여 truncation이 필요할때는 코드의 끝 부분들을 이용하게 만듭니다.\n",
    "\n",
    "    dacon_train_df, dacon_valid_df, dacon_train_label, dacon_valid_label = train_test_split(\n",
    "        train_df,\n",
    "        train_df['problem_num'],\n",
    "        random_state=args.seed,\n",
    "        test_size=0.1,\n",
    "        stratify=filtered_codenet_df['problem_num']\n",
    "    )\n",
    "\n",
    "    dacon_train_df = dacon_train_df.reset_index(drop=True)\n",
    "    dacon_valid_df = dacon_valid_df.reset_index(drop=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "    tokenizer.truncation_side = 'left'\n",
    "\n",
    "    dacon_train_bm25L = get_pairs(dacon_train_df, tokenizer)\n",
    "    dacon_valid_bm25L = get_pairs(dacon_valid_df, tokenizer)\n",
    "\n",
    "    # 생성된 데이터를 저장합니다. => 이 과정까지의 생성 시간이 꽤 오래걸립니다.\n",
    "    dacon_train_bm25L.to_csv(\"./data/\" + \"new_dataset_0607/graph_dacon_train_bm25L.csv\", index=False)\n",
    "    dacon_valid_bm25L.to_csv(\"./data/\" + \"new_dataset_0607/graph_dacon_valid_bm25L.csv\", index=False)\n",
    "    processed_test.to_csv(\"./data/new_dataset_0604/processed_test.csv\", index=False)\n",
    "\n",
    "    codenet_train_df, codenet_valid_df, codenet_train_label, codenet_valid_label = train_test_split(\n",
    "        filtered_codenet_df,\n",
    "        filtered_codenet_df['problem_num'],\n",
    "        random_state=args.seed,\n",
    "        test_size=0.1,\n",
    "        stratify=filtered_codenet_df['problem_num']\n",
    "    )\n",
    "    codenet_train_df = codenet_train_df.reset_index(drop=True)\n",
    "    codenet_valid_df = codenet_valid_df.reset_index(drop=True)\n",
    "\n",
    "    codenet_train_bm25L = get_pairs(codenet_train_df, tokenizer)\n",
    "    codenet_valid_bm25L = get_pairs(codenet_valid_df, tokenizer)\n",
    "\n",
    "    # 생성된 데이터를 저장합니다.\n",
    "    codenet_train_bm25L.to_csv(\"./data/\" + \"new_dataset_0607/graph_codenet_train_bm25L.csv\",\n",
    "                               index=False)\n",
    "    codenet_valid_bm25L.to_csv(\"./data/\" + \"new_dataset_0607/graph_codenet_valid_bm25L.csv\",\n",
    "                               index=False)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(args):\n",
    "\n",
    "    \"\"\"\n",
    "    전처리된 데이터 호출 및 모델을 학습시키는 과정입니다.\n",
    "\n",
    "    순서는 다음과 같습니다.\n",
    "\n",
    "    [1. 전처리 과정에서 생성된 데이터 호출]\n",
    "    1-1. dacon - train: graph_dacon_train_bm25L.csv\n",
    "    1-2, dacon - valid: graph_dacon_valid_bm25L.csv\n",
    "    2-1. codenet - train: graph_codenet_train_bm25L.csv\n",
    "    2-2. codenet - valid: graph_codenet_valid_bm25L.csv\n",
    "\n",
    "    [2. 텐서 생성]\n",
    "    위 데이터를 호출하여 하나의 train_data, valid_data 로 만든 후 텐서를 생성합니다.\n",
    "\n",
    "    - 이 과정까지 각 모델에 대해 전부 진행되기 때문에, 다소 시간이 걸릴 수 있는 점 참고부탁드립니다.\n",
    "    - 파일명: [모델명]_mixed_[train or valid]_[input_ids or attention_masks]_[BM25 or BM25L].pt (ex:) _mixed_train_input_ids_BM25L_0608.pt)\n",
    "\n",
    "    :param args:\n",
    "        --seed 42\n",
    "        --learning_rate 2e-5\n",
    "        --eps 1e-5\n",
    "        --epochs 3\n",
    "        --batch_size 32\n",
    "        --test_batch_size 1048\n",
    "        --save_tensor True\n",
    "        --mode train\n",
    "        --dir_path graphcodebert\n",
    "        --model_name graphcodebert\n",
    "        --process_name code_similarity\n",
    "        --checkpoint_path microsoft/graphcodebert-base\n",
    "    :return: X\n",
    "    \"\"\"\n",
    "\n",
    "    set_seed(args)\n",
    "    setproctitle(args.process_name)\n",
    "\n",
    "    dacon_train_data = pd.read_csv(\"./data/\" + \"new_dataset_0607/graph_dacon_train_bm25L.csv\")\n",
    "    dacon_valid_data = pd.read_csv(\"./data/\" + \"new_dataset_0607/graph_dacon_valid_bm25L.csv\")\n",
    "\n",
    "    codenet_train_data = pd.read_csv(\"./data/\" + \"new_dataset_0607/graph_codenet_train_bm25L.csv\")\n",
    "    codenet_valid_data = pd.read_csv(\"./data/\" + \"new_dataset_0607/graph_codenet_valid_bm25L.csv\")\n",
    "\n",
    "    train_data = pd.concat([dacon_train_data, codenet_train_data], axis=0)\n",
    "    valid_data = pd.concat([dacon_valid_data, codenet_valid_data], axis=0)\n",
    "\n",
    "    # training\n",
    "    c1 = train_data['code1'].values\n",
    "    c2 = train_data['code2'].values\n",
    "    similar = train_data['similar'].values\n",
    "\n",
    "    N = train_data.shape[0]\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "    attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "    labels = np.zeros((N), dtype=int)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "\n",
    "    for i in tqdm(range(N), position=0, leave=True):\n",
    "        try:\n",
    "            cur_c1 = str(c1[i])\n",
    "            cur_c2 = str(c2[i])\n",
    "            encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                      truncation=True)\n",
    "            input_ids[i,] = encoded_input['input_ids']\n",
    "            attention_masks[i,] = encoded_input['attention_mask']\n",
    "            labels[i] = similar[i]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "\n",
    "    # validating\n",
    "    c1 = valid_data['code1'].values\n",
    "    c2 = valid_data['code2'].values\n",
    "    similar = valid_data['similar'].values\n",
    "\n",
    "    N = valid_data.shape[0]\n",
    "\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    valid_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "    valid_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "    valid_labels = np.zeros((N), dtype=int)\n",
    "\n",
    "    for i in tqdm(range(N), position=0, leave=True):\n",
    "        try:\n",
    "            cur_c1 = str(c1[i])\n",
    "            cur_c2 = str(c2[i])\n",
    "            encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                      truncation=True)\n",
    "            valid_input_ids[i,] = encoded_input['input_ids']\n",
    "            valid_attention_masks[i,] = encoded_input['attention_mask']\n",
    "            valid_labels[i] = similar[i]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    if os.path.exists(args.dir_path):\n",
    "        os.makedirs(args.dir_path, exist_ok=True)\n",
    "\n",
    "    print(\"\\n\\nMake tensor\\n\\n\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=int)\n",
    "    attention_masks = torch.tensor(attention_masks, dtype=int)\n",
    "    labels = torch.tensor(labels, dtype=int)\n",
    "\n",
    "    valid_input_ids = torch.tensor(valid_input_ids, dtype=int)\n",
    "    valid_attention_masks = torch.tensor(valid_attention_masks, dtype=int)\n",
    "    valid_labels = torch.tensor(valid_labels, dtype=int)\n",
    "\n",
    "\n",
    "    if args.save_tensor == True:\n",
    "        torch.save(input_ids, \"./data/\" + args.dir_path + \"/\" + args.model_name + '_mixed_train_input_ids_BM25L_0608.pt')\n",
    "        torch.save(attention_masks, \"./data/\" + args.dir_path + \"/\" + args.model_name + '_mixed_train_attention_masks_BM25L_0608.pt')\n",
    "        torch.save(labels, \"./data/\" + args.dir_path + \"/\" + args.model_name + '_mixed_train_labels_BM25L_0608.pt')\n",
    "\n",
    "        torch.save(valid_input_ids, \"./data/\" + args.dir_path + \"/\" + args.model_name + \"_mixed_valid_input_ids_BM25L_0608.pt\")\n",
    "        torch.save(valid_attention_masks, \"./data/\" + args.dir_path + \"/\" + args.model_name + \"mixed_valid_attention_masks_BM25L_0608.pt\")\n",
    "        torch.save(valid_labels, \"./data/\" + args.dir_path + \"/\" + args.model_name + \"mixed_valid_labels_BM25L_0608.pt\")\n",
    "\n",
    "\n",
    "    # Setup training\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    def format_time(elapsed):\n",
    "        elapsed_rounded = int(round((elapsed)))\n",
    "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "    train_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n",
    "\n",
    "    validation_data = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=args.batch_size)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.checkpoint_path)\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=1e-5)  # 아직 이게 정확하지 않음\n",
    "\n",
    "    total_steps = len(train_dataloader) * args.epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    loss_f = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    model.zero_grad()\n",
    "    for i in range(args.epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(i + 1, args.epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        train_loss, train_accuracy = 0, 0\n",
    "        model.train()\n",
    "        for step, batch in tqdm(enumerate(train_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "            if step % 10000 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "                print('  current average loss = {}'.format(\n",
    "                    train_loss / step))  # bot.sendMessage(chat_id=chat_id, text = '  current average loss = {}'.format(train_loss / step))\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            train_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.detach().cpu().numpy()\n",
    "            train_accuracy += flat_accuracy(logits, label_ids)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        avg_train_accuracy = train_accuracy / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(avg_train_accuracy)\n",
    "        print(\"  Average training loss: {0:.8f}\".format(avg_train_loss))\n",
    "        print(\"  Average training accuracy: {0:.8f}\".format(avg_train_accuracy))\n",
    "        print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Validating...\")\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        for step, batch in tqdm(enumerate(validation_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "            logits = outputs[0]\n",
    "            logits = logits.detach().cpu()\n",
    "            label_ids = b_labels.detach().cpu()\n",
    "            val_loss += loss_f(logits, label_ids)\n",
    "\n",
    "            logits = logits.numpy()\n",
    "            label_ids = label_ids.numpy()\n",
    "            val_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "        avg_val_accuracy = val_accuracy / len(validation_dataloader)\n",
    "        avg_val_loss = val_loss / len(validation_dataloader)\n",
    "        val_accuracies.append(avg_val_accuracy)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(\"  Average validation loss: {0:.8f}\".format(avg_val_loss))\n",
    "        print(\"  Average validation accuracy: {0:.8f}\".format(avg_val_accuracy))\n",
    "        print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "        # if np.min(val_losses) == val_losses[-1]:\n",
    "        print(\"saving current best checkpoint\")\n",
    "        torch.save(model.state_dict(), \"./data/\" + args.dir_path + \"/\" + str(i + 1) + \"_mixed_\" + args.model_name + \"_BM25L_0608.pt\")\n",
    "\n",
    "\n",
    "def inference_model(args):\n",
    "    test_data = pd.read_csv(\"./data/new_dataset_0604/processed_test.csv\")\n",
    "\n",
    "    c1 = test_data['code1'].values\n",
    "    c2 = test_data['code2'].values\n",
    "\n",
    "    N = test_data.shape[0]\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    test_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "    test_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    for i in tqdm(range(N), position=0, leave=True):\n",
    "        try:\n",
    "            cur_c1 = str(c1[i])\n",
    "            cur_c2 = str(c2[i])\n",
    "            encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                      truncation=True)\n",
    "            test_input_ids[i,] = encoded_input['input_ids']\n",
    "            test_attention_masks[i,] = encoded_input['attention_mask']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    test_input_ids = torch.tensor(test_input_ids, dtype=int)\n",
    "    test_attention_masks = torch.tensor(test_attention_masks, dtype=int)\n",
    "\n",
    "    if args.save_tensor == True:\n",
    "        torch.save(test_input_ids, \"./data/\" + args.dir_path + \"/\" + \"test_input_ids_0605.pt\")\n",
    "        torch.save(test_attention_masks, \"./data/\" + args.dir_path + \"/\" + \"test_attention_masks_0605.pt\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.checkpoint_path)\n",
    "    PATH = \"./data/\" + args.dir_path + \"/\" + \"1_mixed_\" + args.model_name + \"_BM25L_0608.pt\"\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.cuda()\n",
    "\n",
    "    test_tensor = TensorDataset(test_input_ids, test_attention_masks)\n",
    "    test_sampler = SequentialSampler(test_tensor)\n",
    "    test_dataloader = DataLoader(test_tensor, sampler=test_sampler, batch_size=args.test_batch_size)\n",
    "\n",
    "    submission = pd.read_csv('./data/sample_submission.csv')\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    preds = np.array([])\n",
    "    for step, batch in tqdm(enumerate(test_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu()\n",
    "        _pred = logits.numpy()\n",
    "        pred = np.argmax(_pred, axis=1).flatten()\n",
    "        preds = np.append(preds, pred)\n",
    "\n",
    "    submission['similar'] = preds\n",
    "    submission.to_csv('./data/submission_' + args.model_name + '_0610.csv', index=False)\n",
    "\n",
    "\n",
    "def model_ensemble():\n",
    "    submission = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "    submission_1 = pd.read_csv('./data/submission_graphcodebert_BM25L_0610.csv')\n",
    "    submission_2 = pd.read_csv('./data/submission_CodeBERTaPy_BM25L_0610.csv')\n",
    "    submission_3 = pd.read_csv('./data/submission_codebert_mlm_BM25L_0610.csv')\n",
    "\n",
    "    sub_1 = submission_1['similar']\n",
    "    sub_2 = submission_2['similar']\n",
    "    sub_3 = submission_3['similar']\n",
    "\n",
    "    ensemble_preds = (sub_1 + sub_2 + sub_3) / 3\n",
    "\n",
    "    preds = np.where(ensemble_preds > 0.5, 1, 0)\n",
    "\n",
    "    submission['similar'] = preds\n",
    "\n",
    "    submission.to_csv('./data/submission_ensemble_0610_v2.csv', index=False)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Set arguments.\")\n",
    "# \n",
    "#     parser.add_argument(\"--seed\", default=\"42\", type=int, help=\"Random seed for initialization\")\n",
    "#     parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "#     parser.add_argument(\"--eps\", default=1e-5, type=float, help=\"The initial eps.\")\n",
    "#     parser.add_argument(\"--epochs\", default=3, type=int, help=\"Total number of epochs to train.\")\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=None, help=\"batch_size\")\n",
    "#     parser.add_argument(\"--test_batch_size\", type=int, default=None, help=\"test_batch_size\")\n",
    "# \n",
    "#     parser.add_argument(\"--no_cuda\", default=False, type=bool, help=\"Say True if you don't want to use cuda.\")\n",
    "#     parser.add_argument(\"--ensemble\", default=False, type=bool, help=\"Ensemble.\")\n",
    "#     parser.add_argument(\"--save_tensor\", default=True, type=str, help=\"Save tensor.\")\n",
    "#     parser.add_argument(\"--mode\", default=\"train\", type=str, help=\"When you train the model.\")\n",
    "#     parser.add_argument(\"--dir_path\", default=\"graphcodebert\", type=str, help=\"Save model path.\")\n",
    "#     parser.add_argument(\"--model_name\", default=\"graphcodebert\", type=str, help=\"Model name.\")\n",
    "#     parser.add_argument(\"--process_name\", default=\"code_similarity\", type=str, help=\"process_name.\")\n",
    "#     parser.add_argument(\"--checkpoint_path\", default=\"microsoft/graphcodebert-base\", type=str, help=\"Pre-trained Language Model.\")\n",
    "# \n",
    "#     args = parser.parse_args()\n",
    "# \n",
    "#     if args.mode == \"train\":\n",
    "#         data_preprocess(args)\n",
    "#         train_model(args)\n",
    "#     else:\n",
    "#         inference_model(args)\n",
    "# \n",
    "#     if args.ensemble == True:\n",
    "#         model_ensemble()\n",
    "# \n",
    "#     # CUDA_VISIBLE_DEVICES=0 python code_submission.py --seed 42 --learning_rate 2e-5 --eps 1e-5 --epochs 3 --batch_size 32 --test_batch_size 1048 --save_tensor True --mode train --dir_path graphcodebert --model_name graphcodebert --process_name code_similarity --checkpoint_path microsoft/graphcodebert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"seed\": 42,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"eps\": 1e-5,\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"test_batch_size\": 32,\n",
    "    \"save_tensor\": False,  # argparse의 'store_true' 액션 대응\n",
    "    \"mode\": \"train\",\n",
    "    \"dir_path\": \"graphcodebert\",\n",
    "    \"model_name\": \"graphcodebert\",\n",
    "    \"process_name\": \"code_similarity\",\n",
    "    \"checkpoint_path\": \"microsoft/graphcodebert-base\"\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T06:14:33.355270100Z",
     "start_time": "2024-03-29T06:14:33.326349400Z"
    }
   },
   "id": "b59547731bbb936"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [09:11<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# 데이콘이 제공해준 학습 코드 데이터 데이터프레임 만들기\n",
    "code_folder = \"../data/train_code\"  # 데이콘이 제공해준 학습 데이터 파일의 경로\n",
    "problem_folders = os.listdir(code_folder)\n",
    "preproc_scripts = []\n",
    "problem_nums = []\n",
    "\n",
    "for problem_folder in tqdm(problem_folders):\n",
    "    scripts = os.listdir(os.path.join(code_folder, problem_folder))\n",
    "    problem_num = scripts[0].split('_')[0]\n",
    "    for script in scripts:\n",
    "        script_file = os.path.join(code_folder, problem_folder, script)\n",
    "        preprocessed_script = clean_data(script_file, data_type=\"dir\")\n",
    "        preproc_scripts.append(preprocessed_script)\n",
    "    problem_nums.extend([problem_num] * len(scripts))\n",
    "train_df = pd.DataFrame(data={'code': preproc_scripts, 'problem_num': problem_nums})\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:11:20.619584700Z",
     "start_time": "2024-03-26T12:02:08.998377600Z"
    }
   },
   "id": "aec1c599e9503d9c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595000/595000 [00:25<00:00, 23015.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이콘이 제공해준 테스트 코드 데이터 데이터프레임 만들기\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "code1 = test_df['code1'].values\n",
    "code2 = test_df['code2'].values\n",
    "processed_code1 = []\n",
    "processed_code2 = []\n",
    "for i in tqdm(range(len(code1))):\n",
    "    processed_c1 = clean_data(code1[i], data_type=\"file\")\n",
    "    processed_c2 = clean_data(code2[i], data_type=\"file\")\n",
    "    processed_code1.append(processed_c1)\n",
    "    processed_code2.append(processed_c2)\n",
    "processed_test = pd.DataFrame(list(zip(processed_code1, processed_code2)), columns=[\"code1\", \"code2\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:11:57.954264900Z",
     "start_time": "2024-03-26T12:11:20.620582200Z"
    }
   },
   "id": "6f980023a1ac139a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dacon_train_df, dacon_valid_df, dacon_train_label, dacon_valid_label = train_test_split(\n",
    "    train_df,\n",
    "    train_df['problem_num'],\n",
    "    random_state=args.seed,\n",
    "    test_size=0.1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:11:58.012037400Z",
     "start_time": "2024-03-26T12:11:57.968039700Z"
    }
   },
   "id": "2d0f0add88d8d7af"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 500/500 [4:05:24<00:00, 29.45s/it]  \n",
      "100%|██████████| 500/500 [25:24<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "dacon_train_df = dacon_train_df.reset_index(drop=True)\n",
    "dacon_valid_df = dacon_valid_df.reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "dacon_train_bm25L = get_pairs(dacon_train_df, tokenizer)\n",
    "dacon_valid_bm25L = get_pairs(dacon_valid_df, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T16:48:01.437022900Z",
     "start_time": "2024-03-26T12:11:58.012037400Z"
    }
   },
   "id": "a61674e45e50dae9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:266\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    257\u001B[0m     handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    258\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m     quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    264\u001B[0m )\n\u001B[1;32m--> 266\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:271\u001B[0m, in \u001B[0;36mCSVFormatter._save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_header()\n\u001B[1;32m--> 271\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:309\u001B[0m, in \u001B[0;36mCSVFormatter._save_body\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    308\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m--> 309\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_i\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:320\u001B[0m, in \u001B[0;36mCSVFormatter._save_chunk\u001B[1;34m(self, start_i, end_i)\u001B[0m\n\u001B[0;32m    319\u001B[0m ix \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_index[slicer]\u001B[38;5;241m.\u001B[39m_format_native_types(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_number_format)\n\u001B[1;32m--> 320\u001B[0m \u001B[43mlibwriters\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_csv_rows\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[43m    \u001B[49m\u001B[43mix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnlevels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mwriters.pyx:72\u001B[0m, in \u001B[0;36mpandas._libs.writers.write_csv_rows\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;31mOSError\u001B[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 생성된 데이터를 저장합니다. => 이 과정까지의 생성 시간이 꽤 오래걸립니다.\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mdacon_train_bm25L\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../data/graph_dacon_train_bm25L.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m dacon_valid_bm25L\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/graph_dacon_valid_bm25L.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m processed_test\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/processed_test.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3891\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[0;32m   3893\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3894\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3895\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3899\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3900\u001B[0m )\n\u001B[1;32m-> 3902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3903\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3904\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3905\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3906\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3907\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3908\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3909\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3910\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3911\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3912\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3913\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3914\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m   1131\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m   1134\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m   1135\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1150\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1151\u001B[0m )\n\u001B[1;32m-> 1152\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[0;32m   1155\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:266\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_handle(\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepath_or_buffer,\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    254\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    257\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    258\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    264\u001B[0m     )\n\u001B[1;32m--> 266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\io\\common.py:142\u001B[0m, in \u001B[0;36mIOHandles.__exit__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\pandas\\io\\common.py:134\u001B[0m, in \u001B[0;36mIOHandles.close\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreated_handles\u001B[38;5;241m.\u001B[39mremove(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle)\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handle \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreated_handles:\n\u001B[1;32m--> 134\u001B[0m     \u001B[43mhandle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreated_handles \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mOSError\u001B[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# 생성된 데이터를 저장합니다. => 이 과정까지의 생성 시간이 꽤 오래걸립니다.\n",
    "dacon_train_bm25L.to_csv(\"../data/graph_dacon_train_bm25L.csv\", index=False)\n",
    "dacon_valid_bm25L.to_csv(\"../data/graph_dacon_valid_bm25L.csv\", index=False)\n",
    "processed_test.to_csv(\"../data/processed_test.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T01:48:52.184354200Z",
     "start_time": "2024-03-27T01:28:53.867244900Z"
    }
   },
   "id": "f6a65f60d317d511"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "set_seed(args)\n",
    "setproctitle(args.process_name)\n",
    "\n",
    "# dacon_train_data = pd.read_csv(\"../data/\" + \"new_dataset_0607/graph_dacon_train_bm25L.csv\")\n",
    "# dacon_valid_data = pd.read_csv(\"../data/\" + \"new_dataset_0607/graph_dacon_valid_bm25L.csv\")\n",
    "\n",
    "dacon_train_data = dacon_train_bm25L\n",
    "dacon_valid_data = dacon_valid_bm25L"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T01:53:18.124615400Z",
     "start_time": "2024-03-27T01:53:18.088712800Z"
    }
   },
   "id": "980a7768aca7dfbb"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_data = dacon_train_data\n",
    "valid_data = dacon_valid_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T01:53:25.123890100Z",
     "start_time": "2024-03-27T01:53:25.104175900Z"
    }
   },
   "id": "7b305e504ef837a5"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "sample_size = int(len(train_data) * 0.1)\n",
    "\n",
    "# 무작위로 데이터를 추출합니다.\n",
    "train_data_sample = train_data.sample(n=sample_size, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T01:55:25.137054Z",
     "start_time": "2024-03-27T01:55:15.044042700Z"
    }
   },
   "id": "bcdfc1c80aed1f41"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "sample_size = int(len(valid_data) * 0.1)\n",
    "\n",
    "# 무작위로 데이터를 추출합니다.\n",
    "valid_data_sample = valid_data.sample(n=sample_size, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T01:55:55.819349Z",
     "start_time": "2024-03-27T01:55:55.741817100Z"
    }
   },
   "id": "908d9cc2ba79c0e8"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 생성된 데이터를 저장합니다. => 이 과정까지의 생성 시간이 꽤 오래걸립니다.\n",
    "train_data_sample.to_csv(\"../data/graph_dacon_train_bm25L.csv\", index=False)\n",
    "valid_data_sample.to_csv(\"../data/graph_dacon_valid_bm25L.csv\", index=False)\n",
    "processed_test.to_csv(\"../data/processed_test.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T02:03:06.526723700Z",
     "start_time": "2024-03-27T01:56:42.160526100Z"
    }
   },
   "id": "e760c40631a9836c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7afd1ec518f829c0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dacon_train_data = pd.read_csv(\"../data/\" + \"graph_dacon_train_bm25L.csv\")\n",
    "dacon_valid_data = pd.read_csv(\"../data/\" + \"graph_dacon_valid_bm25L.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:07:42.847413500Z",
     "start_time": "2024-03-28T11:05:06.766828600Z"
    }
   },
   "id": "4ccc410a3a1f2f11"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data = dacon_train_data\n",
    "valid_data = dacon_valid_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:07:42.876368900Z",
     "start_time": "2024-03-28T11:07:42.848411600Z"
    }
   },
   "id": "19281edd6a0b5645"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "sample_size = int(len(train_data) * 0.001)\n",
    "\n",
    "train_data_sample = train_data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "sample_size = int(len(valid_data) * 0.1)\n",
    "\n",
    "valid_data_sample = valid_data.sample(n=sample_size, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:07:43.345683400Z",
     "start_time": "2024-03-28T11:07:42.863371200Z"
    }
   },
   "id": "1fb3a307249c5a17"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_data = train_data_sample\n",
    "valid_data = valid_data_sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:07:43.361486Z",
     "start_time": "2024-03-28T11:07:43.346681Z"
    }
   },
   "id": "64668f286ece0175"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# training\n",
    "c1 = train_data['code1'].values\n",
    "c2 = train_data['code2'].values\n",
    "similar = train_data['similar'].values\n",
    "\n",
    "N = train_data.shape[0]\n",
    "MAX_LEN = 512\n",
    "\n",
    "input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "labels = np.zeros((N), dtype=int)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:07:44.014183500Z",
     "start_time": "2024-03-28T11:07:43.362483Z"
    }
   },
   "id": "af194974406f3044"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10098/10098 [00:24<00:00, 404.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(N), position=0, leave=True):\n",
    "    try:\n",
    "        cur_c1 = str(c1[i])\n",
    "        cur_c2 = str(c2[i])\n",
    "        encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                  truncation=True)\n",
    "        input_ids[i,] = encoded_input['input_ids']\n",
    "        attention_masks[i,] = encoded_input['attention_mask']\n",
    "        labels[i] = similar[i]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:08:09.022228400Z",
     "start_time": "2024-03-28T11:07:44.013187100Z"
    }
   },
   "id": "1af3c9c379dd83fa"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12395/12395 [00:38<00:00, 318.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# validating\n",
    "c1 = valid_data['code1'].values\n",
    "c2 = valid_data['code2'].values\n",
    "similar = valid_data['similar'].values\n",
    "\n",
    "N = valid_data.shape[0]\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "valid_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "valid_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "valid_labels = np.zeros((N), dtype=int)\n",
    "\n",
    "for i in tqdm(range(N), position=0, leave=True):\n",
    "    try:\n",
    "        cur_c1 = str(c1[i])\n",
    "        cur_c2 = str(c2[i])\n",
    "        encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                  truncation=True)\n",
    "        valid_input_ids[i,] = encoded_input['input_ids']\n",
    "        valid_attention_masks[i,] = encoded_input['attention_mask']\n",
    "        valid_labels[i] = similar[i]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:08:47.925709Z",
     "start_time": "2024-03-28T11:08:09.022228400Z"
    }
   },
   "id": "bb36d3189d05480a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Make tensor\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(args.dir_path):\n",
    "    os.makedirs(args.dir_path, exist_ok=True)\n",
    "\n",
    "print(\"\\n\\nMake tensor\\n\\n\")\n",
    "input_ids = torch.tensor(input_ids, dtype=int)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=int)\n",
    "labels = torch.tensor(labels, dtype=int)\n",
    "\n",
    "valid_input_ids = torch.tensor(valid_input_ids, dtype=int)\n",
    "valid_attention_masks = torch.tensor(valid_attention_masks, dtype=int)\n",
    "valid_labels = torch.tensor(valid_labels, dtype=int)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:08:47.985547100Z",
     "start_time": "2024-03-28T11:08:47.919724700Z"
    }
   },
   "id": "1936a3321832dcc2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "if args.save_tensor == True:\n",
    "    torch.save(input_ids, \"./data/\" + args.dir_path + \"/\" + args.model_name + '_mixed_train_input_ids_BM25L_0608.pt')\n",
    "    torch.save(attention_masks, \"./data/\" + args.dir_path + \"/\" + args.model_name + '_mixed_train_attention_masks_BM25L_0608.pt')\n",
    "    torch.save(labels, \"./data/\" + args.dir_path + \"/\" + args.model_name + '_mixed_train_labels_BM25L_0608.pt')\n",
    "\n",
    "    torch.save(valid_input_ids, \"./data/\" + args.dir_path + \"/\" + args.model_name + \"_mixed_valid_input_ids_BM25L_0608.pt\")\n",
    "    torch.save(valid_attention_masks, \"./data/\" + args.dir_path + \"/\" + args.model_name + \"mixed_valid_attention_masks_BM25L_0608.pt\")\n",
    "    torch.save(valid_labels, \"./data/\" + args.dir_path + \"/\" + args.model_name + \"mixed_valid_labels_BM25L_0608.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T11:08:48.028260300Z",
     "start_time": "2024-03-28T11:08:47.987542Z"
    }
   },
   "id": "277b72376ac7f9f6"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\SJ\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 316it [2:29:14, 28.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.60763824\n",
      "  Average training accuracy: 0.64292150\n",
      "  Training epoch took: 2:29:14\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 388it [1:54:27, 17.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average validation loss: 0.45125189\n",
      "  Average validation accuracy: 0.78416413\n",
      "  Training epoch took: 1:54:27\n",
      "saving current best checkpoint\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 316it [1:51:46, 21.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.47668629\n",
      "  Average training accuracy: 0.75681259\n",
      "  Training epoch took: 1:51:46\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 388it [1:10:22, 10.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average validation loss: 0.43954161\n",
      "  Average validation accuracy: 0.76861967\n",
      "  Training epoch took: 1:10:22\n",
      "saving current best checkpoint\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 316it [1:40:53, 19.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average training loss: 0.41589699\n",
      "  Average training accuracy: 0.79513889\n",
      "  Training epoch took: 1:40:54\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 388it [1:10:22, 10.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average validation loss: 0.41744193\n",
      "  Average validation accuracy: 0.79817830\n",
      "  Training epoch took: 1:10:22\n",
      "saving current best checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Setup training\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "train_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n",
    "\n",
    "validation_data = TensorDataset(valid_input_ids, valid_attention_masks, valid_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=args.batch_size)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(args.checkpoint_path)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=1e-5)  # 아직 이게 정확하지 않음\n",
    "\n",
    "total_steps = len(train_dataloader) * args.epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []\n",
    "model.zero_grad()\n",
    "for i in range(args.epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(i + 1, args.epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    model.train()\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "        if step % 10000 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  current average loss = {}'.format(\n",
    "                train_loss / step))  # bot.sendMessage(chat_id=chat_id, text = '  current average loss = {}'.format(train_loss / step))\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        train_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.detach().cpu().numpy()\n",
    "        train_accuracy += flat_accuracy(logits, label_ids)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    avg_train_accuracy = train_accuracy / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_accuracy)\n",
    "    print(\"  Average training loss: {0:.8f}\".format(avg_train_loss))\n",
    "    print(\"  Average training accuracy: {0:.8f}\".format(avg_train_accuracy))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Validating...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    for step, batch in tqdm(enumerate(validation_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = b_labels.detach().cpu()\n",
    "        val_loss += loss_f(logits, label_ids)\n",
    "\n",
    "        logits = logits.numpy()\n",
    "        label_ids = label_ids.numpy()\n",
    "        val_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = val_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = val_loss / len(validation_dataloader)\n",
    "    val_accuracies.append(avg_val_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(\"  Average validation loss: {0:.8f}\".format(avg_val_loss))\n",
    "    print(\"  Average validation accuracy: {0:.8f}\".format(avg_val_accuracy))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # if np.min(val_losses) == val_losses[-1]:\n",
    "    print(\"saving current best checkpoint\")\n",
    "    torch.save(model.state_dict(), \"../data/test.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T21:26:03.819852400Z",
     "start_time": "2024-03-28T11:08:48.002502400Z"
    }
   },
   "id": "258ca5a80369af5e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595000/595000 [21:45<00:00, 455.87it/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Iteration: 0it [00:13, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.28 GiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 18.91 GiB is allocated by PyTorch, and 54.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 54\u001B[0m\n\u001B[0;32m     51\u001B[0m b_input_ids, b_input_mask \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 54\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb_input_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mb_input_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m logits \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     57\u001B[0m logits \u001B[38;5;241m=\u001B[39m logits\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1198\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   1193\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1198\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1199\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1203\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1205\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1206\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1208\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1209\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1210\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    826\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m    828\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    829\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    830\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    833\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m    834\u001B[0m )\n\u001B[1;32m--> 835\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    836\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    837\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    838\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    840\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    847\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    848\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    513\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    514\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    515\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    521\u001B[0m         output_attentions,\n\u001B[0;32m    522\u001B[0m     )\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 524\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    534\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:413\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    403\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    410\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    411\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[0;32m    412\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 413\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    419\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    420\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    422\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:340\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    331\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    332\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    338\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    339\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m--> 340\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    345\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    346\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    349\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[0;32m    350\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:260\u001B[0m, in \u001B[0;36mRobertaSelfAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    257\u001B[0m         relative_position_scores_key \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbhrd,lrd->bhlr\u001B[39m\u001B[38;5;124m\"\u001B[39m, key_layer, positional_embedding)\n\u001B[0;32m    258\u001B[0m         attention_scores \u001B[38;5;241m=\u001B[39m attention_scores \u001B[38;5;241m+\u001B[39m relative_position_scores_query \u001B[38;5;241m+\u001B[39m relative_position_scores_key\n\u001B[1;32m--> 260\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mattention_scores\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_head_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    262\u001B[0m     \u001B[38;5;66;03m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001B[39;00m\n\u001B[0;32m    263\u001B[0m     attention_scores \u001B[38;5;241m=\u001B[39m attention_scores \u001B[38;5;241m+\u001B[39m attention_mask\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 12.28 GiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 18.91 GiB is allocated by PyTorch, and 54.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"../data/processed_test.csv\")\n",
    "\n",
    "c1 = test_data['code1'].values\n",
    "c2 = test_data['code2'].values\n",
    "\n",
    "N = test_data.shape[0]\n",
    "MAX_LEN = 512\n",
    "\n",
    "test_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "test_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "for i in tqdm(range(N), position=0, leave=True):\n",
    "    try:\n",
    "        cur_c1 = str(c1[i])\n",
    "        cur_c2 = str(c2[i])\n",
    "        encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                  truncation=True)\n",
    "        test_input_ids[i,] = encoded_input['input_ids']\n",
    "        test_attention_masks[i,] = encoded_input['attention_mask']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids, dtype=int)\n",
    "test_attention_masks = torch.tensor(test_attention_masks, dtype=int)\n",
    "\n",
    "if args.save_tensor == True:\n",
    "    torch.save(test_input_ids, \"./data/\" + args.dir_path + \"/\" + \"test_input_ids_0605.pt\")\n",
    "    torch.save(test_attention_masks, \"./data/\" + args.dir_path + \"/\" + \"test_attention_masks_0605.pt\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(args.checkpoint_path)\n",
    "PATH = \"../data/test.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.cuda()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T06:14:04.545138200Z",
     "start_time": "2024-03-29T05:51:44.520495900Z"
    }
   },
   "id": "61f96710ba5aff53"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 18594it [3:17:01,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tensor = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_sampler = SequentialSampler(test_tensor)\n",
    "test_dataloader = DataLoader(test_tensor, sampler=test_sampler, batch_size=args.test_batch_size)\n",
    "\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "preds = np.array([])\n",
    "for step, batch in tqdm(enumerate(test_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu()\n",
    "    _pred = logits.numpy()\n",
    "    pred = np.argmax(_pred, axis=1).flatten()\n",
    "    preds = np.append(preds, pred)\n",
    "\n",
    "submission['similar'] = preds\n",
    "submission.to_csv('../submission/bert_1', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:31:40.901665300Z",
     "start_time": "2024-03-29T06:14:38.128007800Z"
    }
   },
   "id": "32d08d6ffa5d8c67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3acb5aceee7e56ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
