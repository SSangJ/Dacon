{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-17T08:32:44.773144100Z",
     "start_time": "2024-03-17T08:32:20.327405400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (Dataset,\n",
    "                              DataLoader,\n",
    "                              RandomSampler,\n",
    "                              SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import (get_scheduler,\n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_recall_curve,\n",
    "                             f1_score,\n",
    "                             auc)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import math\n",
    "\n",
    "from datasets import load_metric, load_dataset, Dataset, concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import (AutoConfig,\n",
    "                          AutoTokenizer,\n",
    "                          RobertaForSequenceClassification,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          DataCollatorWithPadding,\n",
    "                          EarlyStoppingCallback)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T08:54:53.778823900Z",
     "start_time": "2024-03-17T08:54:53.732946600Z"
    }
   },
   "id": "872ed0d33b8b78e0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T08:54:54.327315800Z",
     "start_time": "2024-03-17T08:54:54.263581200Z"
    }
   },
   "id": "7383450cfb6fce07"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    label_indices = list(range(3))\n",
    "    f1 = f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "    return {'micro f1 score': f1}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T08:54:55.034035800Z",
     "start_time": "2024-03-17T08:54:55.023066Z"
    }
   },
   "id": "744f7c1d9ac2dcdd"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/9013 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d6084660dcc48dbb3bdbd202f78aa35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# k-fold를 위해 나누어져있는 dataset을 다시 합쳤습니다.\n",
    "# 기존 코드를 사용하여 데이터셋 로드 및 결합\n",
    "train_dset = load_dataset(\"csv\", data_files=\"../data/train_data_lv1.csv\")['train']\n",
    "validation_dset = load_dataset(\"csv\", data_files=\"../data/valid_data_lv1.csv\")['train']\n",
    "rawdataset = concatenate_datasets([train_dset, validation_dset])\n",
    "\n",
    "# 전체 데이터셋의 10%만을 유지하기 위해 train_test_split 사용\n",
    "rawdataset = rawdataset.train_test_split(test_size=0.1, seed=42)['test']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "# Tokenize\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding=True, max_length=512, truncation=True)\n",
    "    if 'similar' in examples:\n",
    "        outputs[\"labels\"] = examples[\"similar\"]\n",
    "    return outputs\n",
    "dset = rawdataset.map(example_fn, remove_columns=train_dset.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T08:55:17.923777600Z",
     "start_time": "2024-03-17T08:54:56.694852400Z"
    }
   },
   "id": "efaa03613cb7686"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(args):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    config =  AutoConfig.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "    config.num_labels = 2\n",
    "\n",
    "\n",
    "    gap = int(len(dset) / args.k_fold)\n",
    "\n",
    "    for i in range(args.k_fold):\n",
    "\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\", config=config).to(device)\n",
    "\n",
    "        print('\\n%dth Training' %(i+1))\n",
    "\n",
    "        output_dir = args.output_dir + '_' + str(i+1)\n",
    "        logging_dir = args.logging_dir + '_' + str(i+1)\n",
    "\n",
    "        # trainingset, validset 구성\n",
    "        total_size = len(dset)\n",
    "        total_ids = list(range(total_size))\n",
    "        del_ids = list(range(i*gap, (i+1)*gap))\n",
    "        training_ids = set(total_ids) - set(del_ids)\n",
    "\n",
    "        training_dset = dset.select(list(training_ids))\n",
    "        eval_dset = dset.select(del_ids)\n",
    "\n",
    "        # Training Arguments -> Graphcodebert 깃허브를 참고하여 설정했습니다.\n",
    "        args.max_steps=args.epochs*len(dset)\n",
    "        args.save_steps=len(dset)//10\n",
    "        args.warmup_steps = args.max_steps//5\n",
    "\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=args.output_dir,                         # output directory\n",
    "            overwrite_output_dir=True,                          # overwrite output directory\n",
    "            save_total_limit=5,                                 # number of total save model.\n",
    "            save_steps=args.save_steps,                         # model saving step.\n",
    "            num_train_epochs=args.epochs,                       # total number of training epochs\n",
    "            learning_rate=args.lr,                              # learning_rate\n",
    "            per_device_train_batch_size=args.train_batch_size,  # batch size per device during training\n",
    "            per_device_eval_batch_size=args.eval_batch_size,    # batch size for evaluation\n",
    "            warmup_steps=args.warmup_steps,                     # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=args.weight_decay,                     # strength of weight decay\n",
    "            logging_dir=args.logging_dir,                       # directory for storing logs\n",
    "            logging_steps=args.logging_steps,                   # log saving step.\n",
    "            evaluation_strategy=args.evaluation_strategy,       # evaluation strategy to adopt during training\n",
    "            eval_steps=args.eval_steps,                         # evaluation step.\n",
    "            load_best_model_at_end = True, # for earlystopping\n",
    "            save_strategy = 'steps', # for earlystopping\n",
    "            logging_strategy = 'steps', # for earlystopping\n",
    "            gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        )\n",
    "\n",
    "        collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=512)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,                         # the instantiated Transformers model to be trained\n",
    "            args=training_args,                  # training arguments, defined above\n",
    "            train_dataset=training_dset,            # training dataset\n",
    "            eval_dataset=eval_dset,        # evaluation dataset\n",
    "            data_collator=collator,              # collator\n",
    "            compute_metrics=compute_metrics,      # define metrics function -> micro f1\n",
    "            callbacks = [EarlyStoppingCallback(early_stopping_patience=10)],\n",
    "        )\n",
    "\n",
    "        # -- Training\n",
    "        print('Training Strats')\n",
    "        trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T08:55:17.938636200Z",
     "start_time": "2024-03-17T08:55:17.924774700Z"
    }
   },
   "id": "ef4e568ac7943507"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "    'output_dir': './DACON',\n",
    "    'logging_dir': './DACON',\n",
    "    'lr': 2e-5,\n",
    "    'epochs': 3,\n",
    "    'train_batch_size': 4,\n",
    "    'weight_decay': 0.0,\n",
    "    'warmup_steps': 0,\n",
    "    'gradient_accumulation_steps':2,\n",
    "    'eval_batch_size': 8,\n",
    "    'k_fold':5,\n",
    "    'evaluation_strategy': 'steps',\n",
    "    'save_steps': 500,\n",
    "    'logging_steps': 1000,\n",
    "    'eval_steps':901,\n",
    "    'max_steps':-1\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T08:55:40.138564200Z",
     "start_time": "2024-03-17T08:55:40.125599Z"
    }
   },
   "id": "fd6b7d3ffe9b26cf"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1th Training\n",
      "Training Strats\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2703 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2th Training\n",
      "Training Strats\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2703 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./DACON\\checkpoint-901 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./DACON\\checkpoint-1802 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./DACON\\checkpoint-2703 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3th Training\n",
      "Training Strats\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2703 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./DACON\\checkpoint-901 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./DACON\\checkpoint-1802 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./DACON\\checkpoint-2703 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4th Training\n",
      "Training Strats\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2703 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./DACON\\checkpoint-901 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./DACON\\checkpoint-1802 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./DACON\\checkpoint-2703 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5th Training\n",
      "Training Strats\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2703 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./DACON\\checkpoint-901 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./DACON\\checkpoint-1802 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./DACON\\checkpoint-2703 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train(args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T13:32:50.829146900Z",
     "start_time": "2024-03-17T08:55:40.402156600Z"
    }
   },
   "id": "2f0e1a41ec3f2034"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def preprocess_script(code):\n",
    "    # code1과 code2에 대한 처리를 함수로 간소화\n",
    "    def preprocess_code(source_code):\n",
    "        new_code = deque()\n",
    "        block_comment = False  # 블록 주석 처리를 위한 플래그\n",
    "        for line in source_code.split('\\n'):\n",
    "            # 블록 주석 시작\n",
    "            if '/*' in line:\n",
    "                block_comment = True\n",
    "                line = line[:line.index('/*')]\n",
    "            # 블록 주석 끝\n",
    "            if '*/' in line:\n",
    "                block_comment = False\n",
    "                line = line[line.index('*/')+2:]\n",
    "            if block_comment or line.strip().startswith('//'):\n",
    "                continue  # 블록 주석 중이거나 한 줄 주석이면 건너뜀\n",
    "\n",
    "            line = line.rstrip()\n",
    "            # 한 줄 주석 처리\n",
    "            if '//' in line:\n",
    "                line = line[:line.index('//')]\n",
    "            line = line.replace('\\n', '')  # 개행 문자 삭제\n",
    "            line = line.replace('    ', '\\t')  # 공백 4칸을 탭으로 변환\n",
    "\n",
    "            if line == '':  # 전처리 후 빈 라인은 건너뜀\n",
    "                continue\n",
    "\n",
    "            new_code.append(line)\n",
    " \n",
    "    return code\n",
    "\n",
    "\n",
    "def example_fn(examples):\n",
    "    outputs = tokenizer(examples['code1'], examples['code2'], padding=True, max_length=512, truncation=True)\n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:58:51.805559500Z",
     "start_time": "2024-03-19T10:58:51.779117900Z"
    }
   },
   "id": "3c6289d7f2710d95"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "testdataset = load_dataset(\"csv\", data_files='../data/test.csv')['train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:58:53.462398800Z",
     "start_time": "2024-03-19T10:58:52.370183800Z"
    }
   },
   "id": "6931f06e933b3377"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "tokenizer.truncation_side = 'left'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:58:54.164767700Z",
     "start_time": "2024-03-19T10:58:53.464393400Z"
    }
   },
   "id": "7219cae360dabda6"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/595000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "767309586c06443182a932b3224012e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "import re\n",
    "preprocessed = testdataset.map(preprocess_script)\n",
    "test_dataset = preprocessed.map(example_fn, remove_columns=['code1', 'code2','pair_id'])\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T11:53:25.801347900Z",
     "start_time": "2024-03-19T11:30:36.671571100Z"
    }
   },
   "id": "3f7700720bc6c7f3"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_dataset,\n",
    "                        batch_size=16,\n",
    "                        shuffle=False,\n",
    "                        collate_fn = collator\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T11:53:30.912510700Z",
     "start_time": "2024-03-19T11:53:30.891567Z"
    }
   },
   "id": "d9c5d1ef4051e1c6"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "first_batch = next(iter(testloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T00:41:26.813036400Z",
     "start_time": "2024-03-20T00:41:26.771068500Z"
    }
   },
   "id": "546fc5e8db64444a"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([16, 512])"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch['input_ids'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T00:41:53.017608500Z",
     "start_time": "2024-03-20T00:41:53.001621500Z"
    }
   },
   "id": "5ac07a67d6ba96eb"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "load_path = f'./DACON/checkpoint-2703/optimizer.pt'\n",
    "# model.load_state_dict(torch.load(load_path,map_location=torch.device('cpu')))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:02:49.103970200Z",
     "start_time": "2024-03-19T10:02:47.719888800Z"
    }
   },
   "id": "4802fcc1e04e1d00"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/37188 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c2c7854168f431796b47be616d70736"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "progress_bar = tqdm(enumerate(testloader), total=len(testloader), leave=True, position=0,)\n",
    "for i, data in progress_bar:\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            data['input_ids'].to(device),\n",
    "            data['attention_mask'].to(device),\n",
    "        )\n",
    "        logits=logits.logits\n",
    "    if i==0:\n",
    "        one_fold_logits = logits\n",
    "    else:\n",
    "        one_fold_logits = torch.cat([one_fold_logits,logits],dim=0)\n",
    "\n",
    "# torch tensor를 저장하기 위한 numpy 변환\n",
    "one_fold_logits = one_fold_logits.squeeze(0).detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T15:51:54.142273200Z",
     "start_time": "2024-03-19T11:53:51.321037500Z"
    }
   },
   "id": "9fd0e552a1038888"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "a = np.argmax(one_fold_logits,axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T00:24:59.747595400Z",
     "start_time": "2024-03-20T00:24:59.728645200Z"
    }
   },
   "id": "5b987d6742dc48fb"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.08194216,  0.08257993],\n       [-0.04088554, -0.00806557],\n       [-0.08088842,  0.03998123],\n       [-0.09444278,  0.0371286 ],\n       [-0.05098515,  0.02198036],\n       [-0.04448672,  0.05920207],\n       [-0.10218832,  0.00994879],\n       [-0.06491666, -0.01072896],\n       [-0.02801997,  0.05661651],\n       [-0.10346632,  0.05109083],\n       [-0.04204335, -0.02677844],\n       [-0.05192484,  0.01970526],\n       [-0.02836405,  0.06451653],\n       [-0.12566161,  0.07503513],\n       [-0.09584039,  0.01191681],\n       [-0.05152438, -0.03284322],\n       [-0.05662401,  0.04437123],\n       [-0.0765712 ,  0.10035465],\n       [-0.07291934,  0.08909668],\n       [-0.05363311, -0.0034636 ],\n       [-0.07749471,  0.00828263],\n       [-0.06051099,  0.01428877],\n       [-0.04158355,  0.05776763],\n       [-0.09222958,  0.03402672],\n       [-0.0589044 ,  0.06194304],\n       [-0.03134386,  0.13994384],\n       [-0.06475163,  0.04282352],\n       [-0.05242182,  0.03854364],\n       [-0.08807448,  0.05272102],\n       [-0.03242556,  0.05895062],\n       [-0.06913341,  0.07879998],\n       [-0.05125226,  0.03072975],\n       [-0.05782944,  0.07955723],\n       [-0.02625891,  0.0425765 ],\n       [-0.07243691, -0.01682622],\n       [-0.09312022,  0.06230802],\n       [-0.08943538,  0.02362404],\n       [-0.05001047,  0.07573128],\n       [-0.095596  ,  0.05634707],\n       [-0.04705736,  0.08493736],\n       [-0.08998681, -0.01907104],\n       [-0.03359008,  0.08253488],\n       [-0.04881259,  0.04650252],\n       [-0.0676896 ,  0.05962619],\n       [-0.09841763,  0.05409066],\n       [-0.11289399, -0.01558273],\n       [-0.07907075, -0.00981288],\n       [-0.08517859,  0.0729333 ],\n       [-0.09089041,  0.02594397],\n       [-0.06688605,  0.04993397],\n       [-0.05559667,  0.0429352 ],\n       [-0.05037264,  0.06949453],\n       [-0.07346337,  0.05273211],\n       [-0.08945554,  0.04615238],\n       [-0.10031836,  0.06266359],\n       [-0.09016914,  0.07887204],\n       [-0.02203937,  0.07087856],\n       [-0.06997879,  0.03507414],\n       [-0.05614232,  0.08170773],\n       [-0.09648242,  0.01475737],\n       [-0.06652632,  0.02941876],\n       [-0.07718852,  0.06585377],\n       [-0.07771452,  0.09556741],\n       [-0.08626304,  0.03704098],\n       [-0.03056555,  0.02027948],\n       [-0.0415775 ,  0.11240231],\n       [-0.04025774,  0.06294664],\n       [-0.09572443,  0.02140036],\n       [-0.08720478,  0.06436174],\n       [-0.05787797,  0.04117008],\n       [-0.03951464,  0.02682527],\n       [-0.09177338,  0.07675066],\n       [-0.05624368,  0.0897829 ],\n       [-0.07346711, -0.01963655],\n       [-0.06763451,  0.02365782],\n       [-0.07723213, -0.00919912],\n       [-0.07727174,  0.03233413],\n       [-0.03225813,  0.01070846],\n       [-0.10209665,  0.06490996],\n       [-0.05978573,  0.0008298 ],\n       [-0.04176375,  0.06518411],\n       [-0.03404184,  0.02247493],\n       [-0.0613347 ,  0.03610627],\n       [-0.0241508 ,  0.05085781],\n       [-0.06114433,  0.06994456],\n       [-0.02703332,  0.03319515],\n       [-0.10162339,  0.04192197],\n       [-0.01644367,  0.06087858],\n       [-0.02561313,  0.04224796],\n       [-0.08624239,  0.03138833],\n       [-0.05619876,  0.06773965],\n       [-0.04941872,  0.06208705],\n       [-0.03123338,  0.06580585],\n       [-0.04894155,  0.11932693],\n       [-0.0323105 ,  0.02860617],\n       [-0.10289101,  0.03710195],\n       [-0.09224134,  0.10801768],\n       [-0.05104931,  0.04386245],\n       [-0.04272244,  0.00682938],\n       [-0.09248059,  0.01899154]], dtype=float32)"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_fold_logits[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T00:29:38.941219800Z",
     "start_time": "2024-03-20T00:29:38.921273Z"
    }
   },
   "id": "451b16aece697b93"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T00:48:49.380531600Z",
     "start_time": "2024-03-20T00:48:49.162608700Z"
    }
   },
   "id": "8a274942ea9d7c52"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "submission['similar'] = a"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T00:48:49.395491400Z",
     "start_time": "2024-03-20T00:48:49.377539200Z"
    }
   },
   "id": "70c1d27fc45ebc85"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "            pair_id  similar\n0       TEST_000000        1\n1       TEST_000001        0\n2       TEST_000002        1\n3       TEST_000003        1\n4       TEST_000004        1\n...             ...      ...\n594995  TEST_594995        1\n594996  TEST_594996        1\n594997  TEST_594997        1\n594998  TEST_594998        1\n594999  TEST_594999        1\n\n[595000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pair_id</th>\n      <th>similar</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TEST_000000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TEST_000001</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TEST_000002</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TEST_000003</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TEST_000004</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>594995</th>\n      <td>TEST_594995</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>594996</th>\n      <td>TEST_594996</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>594997</th>\n      <td>TEST_594997</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>594998</th>\n      <td>TEST_594998</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>594999</th>\n      <td>TEST_594999</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>595000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T00:48:49.998488300Z",
     "start_time": "2024-03-20T00:48:49.980535800Z"
    }
   },
   "id": "2935881d81de9827"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "submission.to_csv('../submission/codegraph_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T00:48:57.043607400Z",
     "start_time": "2024-03-20T00:48:56.241343100Z"
    }
   },
   "id": "9cc64dfa3cef00fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "23a9d74015910917"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
