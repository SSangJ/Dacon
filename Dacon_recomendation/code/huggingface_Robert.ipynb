{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:35:59.377008Z",
     "start_time": "2024-06-06T09:35:58.111075500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "view_log_train = pd.read_csv('../data/view_log.csv')\n",
    "article_info = pd.read_csv('../data/article_info.csv')\n",
    "submission = pd.read_csv('../submission/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "       articleID                                              Title  \\\n0   ARTICLE_0000                       19 Tips For Everyday Git Use   \n1   ARTICLE_0001  Intel buys computer vision startup Itseez to i...   \n2   ARTICLE_0002       Practical End-to-End Testing with Protractor   \n3   ARTICLE_0003  Corporate venture growth in Brazil is another ...   \n4   ARTICLE_0004  Cross-channel user experiences with Drupal (aw...   \n5   ARTICLE_0005  Conheça o Relatório de Informações do Público ...   \n6   ARTICLE_0006                  Setting Up HTTP(S) Load Balancing   \n7   ARTICLE_0007          The 200 billion dollar chatbot disruption   \n8   ARTICLE_0008  Google's artificial intelligence is going in t...   \n9   ARTICLE_0009     Spotify UI built with HTML / CSS - Freebiesbug   \n10  ARTICLE_0010                                     Blog | Niantic   \n11  ARTICLE_0011     Microsoft adds Python to deep learning toolkit   \n12  ARTICLE_0012  FH fecha contrato de OEM com SAP para entrega ...   \n13  ARTICLE_0013  Campaigns Are Dead. Modern Marketing Is a Data...   \n14  ARTICLE_0014  Machine learning at the museum: this week on G...   \n15  ARTICLE_0015  TF-Slim: A high level library to define comple...   \n16  ARTICLE_0016                      The new-world insurance agent   \n17  ARTICLE_0017  Margaret Gould Stewart: How giant websites des...   \n18  ARTICLE_0018  Microsoft comprará LinkedIn por 26,2 bilhões d...   \n19  ARTICLE_0019  The Digital Transformation Playbook: Rethink Y...   \n20  ARTICLE_0020  Udacity abre código de simulador de carro autô...   \n21  ARTICLE_0021  Em Paris, livraria troca estoque por máquina q...   \n22  ARTICLE_0022  Google's AI DeepMind Turns its Gaze to Hearths...   \n23  ARTICLE_0023  Conheça o GoBox, marketplace do Grupo Abril - ...   \n24  ARTICLE_0024                                 Hello, TensorFlow!   \n25  ARTICLE_0025  Governo Dilma é desaprovado por 69% e aprovado...   \n26  ARTICLE_0026  Sua empresa já tem um \"Open Innovation Lab\" ? ...   \n27  ARTICLE_0027               The Biggest list of 35 FREE UX Books   \n28  ARTICLE_0028  Google I/O 2016: Android's failure to innovate...   \n29  ARTICLE_0029                    Google+ is five years old today   \n30  ARTICLE_0030  Large CPGs are under attack by startups... and...   \n31  ARTICLE_0031  New Asia Trend Briefing from TrendWatching | S...   \n32  ARTICLE_0032  Honda fortalece estratégia para desenvolver te...   \n33  ARTICLE_0033  Java 8 Streams - A Deeper Approach About Perfo...   \n34  ARTICLE_0034               E quando o cliente diz \"Não gostei\"?   \n\n                                              Content Format Language  \\\n0   I've been using git full time for the past 4 y...   HTML       en   \n1   Intel has acquired computer vision and machine...   HTML       en   \n2   One of the reasons AngularJS is so great to wo...   HTML       en   \n3   Despite recent positive news and a renewed int...   HTML       en   \n4   Last year around this time, I wrote that The B...   HTML       en   \n5   Já havíamos falado no blog sobre como conhecer...   HTML       pt   \n6   HTTP(S) load balancing provides global load ba...   HTML       en   \n7   In 2014, Facebook acquired WhatsApp for $19 bi...   HTML       en   \n8   Artificial intelligence sounds cool in theory,...   HTML       en   \n9   Here is an interesting experiment about redesi...   HTML       en   \n10  With Google Cloud product and engineering team...   HTML       en   \n11  Microsoft is adding Python language support to...   HTML       en   \n12  Colaboração A FH firmou contrato de OEM (sigla...   HTML       pt   \n13  Consumers and Marketers Want the Same Thing --...   HTML       en   \n14  Is there a limit to what you can do with machi...   HTML       en   \n15  Earlier this year, we released a TensorFlow im...   HTML       en   \n16  Silicon Valley is building the new-world insur...   HTML       en   \n17  Facebook's \"like\" and \"share\" buttons are seen...  VIDEO       en   \n18  São Paulo - A Microsoft anunciou hoje que comp...   HTML       pt   \n19  It can feel difficult to keep up at times, wha...   HTML       en   \n20  Várias empresas automotivas já estão desenvolv...   HTML       pt   \n21  Gauthier Charrier, estudante de design gráfico...   HTML       pt   \n22  What will it be used for next? Researchers at ...   HTML       en   \n23  Katia Beauchamp e Hayley Barna, fundadoras da ...   HTML       pt   \n24  Braided river. (source: National Park Service,...   HTML       en   \n25  Pesquisa Ibope encomendada pela Confederação N...   HTML       pt   \n26  Devem brotar, por dia, centenas de artigos nas...   HTML       pt   \n27  We've pulled together the biggest list of free...   HTML       en   \n28  Rather than focusing on the incremental innova...   HTML       en   \n29  Somehow, don't ask me how , Google's zombie of...   HTML       en   \n30  Consumer packaged goods are big business - val...   HTML       en   \n31  The online world has allowed for faster, easie...   HTML       en   \n32  De acordo com a empresa, o local, previsto par...   HTML       pt   \n33  Introduction Java 8 was released almost three ...   HTML       en   \n34  Dez entre dez webdesigners se tremem ao ouvir ...   HTML       pt   \n\n       userID userCountry userRegion  \n0   USER_0683         NaN        NaN  \n1   USER_1129         NaN        NaN  \n2   USER_0256         NaN        NaN  \n3   USER_1304         NaN        NaN  \n4   USER_0336         NaN        NaN  \n5   USER_1324         NaN        NaN  \n6   USER_1304         NaN        NaN  \n7   USER_1304         NaN        NaN  \n8   USER_0827         NaN        NaN  \n9   USER_1421         NaN        NaN  \n10  USER_0674          BR         SP  \n11  USER_0222          BR         SP  \n12  USER_0113         NaN        NaN  \n13  USER_0798         NaN        NaN  \n14  USER_0336         NaN        NaN  \n15  USER_0222         NaN        NaN  \n16  USER_1304         NaN        NaN  \n17  USER_0215         NaN        NaN  \n18  USER_0063         NaN        NaN  \n19  USER_1069         NaN        NaN  \n20  USER_1063          BR         SP  \n21  USER_0878         NaN        NaN  \n22  USER_0222         NaN        NaN  \n23  USER_0113         NaN        NaN  \n24  USER_0873         NaN        NaN  \n25  USER_0952         NaN        NaN  \n26  USER_1110         NaN        NaN  \n27  USER_0947         NaN        NaN  \n28  USER_0222         NaN        NaN  \n29  USER_1179         NaN        NaN  \n30  USER_1420          BR         SP  \n31  USER_1063         NaN        NaN  \n32  USER_0603         NaN        NaN  \n33  USER_0657          BR         SP  \n34  USER_1183         NaN        NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>articleID</th>\n      <th>Title</th>\n      <th>Content</th>\n      <th>Format</th>\n      <th>Language</th>\n      <th>userID</th>\n      <th>userCountry</th>\n      <th>userRegion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ARTICLE_0000</td>\n      <td>19 Tips For Everyday Git Use</td>\n      <td>I've been using git full time for the past 4 y...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ARTICLE_0001</td>\n      <td>Intel buys computer vision startup Itseez to i...</td>\n      <td>Intel has acquired computer vision and machine...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1129</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ARTICLE_0002</td>\n      <td>Practical End-to-End Testing with Protractor</td>\n      <td>One of the reasons AngularJS is so great to wo...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0256</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ARTICLE_0003</td>\n      <td>Corporate venture growth in Brazil is another ...</td>\n      <td>Despite recent positive news and a renewed int...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1304</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ARTICLE_0004</td>\n      <td>Cross-channel user experiences with Drupal (aw...</td>\n      <td>Last year around this time, I wrote that The B...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0336</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ARTICLE_0005</td>\n      <td>Conheça o Relatório de Informações do Público ...</td>\n      <td>Já havíamos falado no blog sobre como conhecer...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_1324</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ARTICLE_0006</td>\n      <td>Setting Up HTTP(S) Load Balancing</td>\n      <td>HTTP(S) load balancing provides global load ba...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1304</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ARTICLE_0007</td>\n      <td>The 200 billion dollar chatbot disruption</td>\n      <td>In 2014, Facebook acquired WhatsApp for $19 bi...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1304</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ARTICLE_0008</td>\n      <td>Google's artificial intelligence is going in t...</td>\n      <td>Artificial intelligence sounds cool in theory,...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0827</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ARTICLE_0009</td>\n      <td>Spotify UI built with HTML / CSS - Freebiesbug</td>\n      <td>Here is an interesting experiment about redesi...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1421</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ARTICLE_0010</td>\n      <td>Blog | Niantic</td>\n      <td>With Google Cloud product and engineering team...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0674</td>\n      <td>BR</td>\n      <td>SP</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ARTICLE_0011</td>\n      <td>Microsoft adds Python to deep learning toolkit</td>\n      <td>Microsoft is adding Python language support to...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0222</td>\n      <td>BR</td>\n      <td>SP</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>ARTICLE_0012</td>\n      <td>FH fecha contrato de OEM com SAP para entrega ...</td>\n      <td>Colaboração A FH firmou contrato de OEM (sigla...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_0113</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ARTICLE_0013</td>\n      <td>Campaigns Are Dead. Modern Marketing Is a Data...</td>\n      <td>Consumers and Marketers Want the Same Thing --...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0798</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>ARTICLE_0014</td>\n      <td>Machine learning at the museum: this week on G...</td>\n      <td>Is there a limit to what you can do with machi...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0336</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>ARTICLE_0015</td>\n      <td>TF-Slim: A high level library to define comple...</td>\n      <td>Earlier this year, we released a TensorFlow im...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0222</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>ARTICLE_0016</td>\n      <td>The new-world insurance agent</td>\n      <td>Silicon Valley is building the new-world insur...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1304</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>ARTICLE_0017</td>\n      <td>Margaret Gould Stewart: How giant websites des...</td>\n      <td>Facebook's \"like\" and \"share\" buttons are seen...</td>\n      <td>VIDEO</td>\n      <td>en</td>\n      <td>USER_0215</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>ARTICLE_0018</td>\n      <td>Microsoft comprará LinkedIn por 26,2 bilhões d...</td>\n      <td>São Paulo - A Microsoft anunciou hoje que comp...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_0063</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>ARTICLE_0019</td>\n      <td>The Digital Transformation Playbook: Rethink Y...</td>\n      <td>It can feel difficult to keep up at times, wha...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1069</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>ARTICLE_0020</td>\n      <td>Udacity abre código de simulador de carro autô...</td>\n      <td>Várias empresas automotivas já estão desenvolv...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_1063</td>\n      <td>BR</td>\n      <td>SP</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>ARTICLE_0021</td>\n      <td>Em Paris, livraria troca estoque por máquina q...</td>\n      <td>Gauthier Charrier, estudante de design gráfico...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_0878</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>ARTICLE_0022</td>\n      <td>Google's AI DeepMind Turns its Gaze to Hearths...</td>\n      <td>What will it be used for next? Researchers at ...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0222</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>ARTICLE_0023</td>\n      <td>Conheça o GoBox, marketplace do Grupo Abril - ...</td>\n      <td>Katia Beauchamp e Hayley Barna, fundadoras da ...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_0113</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>ARTICLE_0024</td>\n      <td>Hello, TensorFlow!</td>\n      <td>Braided river. (source: National Park Service,...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0873</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>ARTICLE_0025</td>\n      <td>Governo Dilma é desaprovado por 69% e aprovado...</td>\n      <td>Pesquisa Ibope encomendada pela Confederação N...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_0952</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>ARTICLE_0026</td>\n      <td>Sua empresa já tem um \"Open Innovation Lab\" ? ...</td>\n      <td>Devem brotar, por dia, centenas de artigos nas...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_1110</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>ARTICLE_0027</td>\n      <td>The Biggest list of 35 FREE UX Books</td>\n      <td>We've pulled together the biggest list of free...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0947</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>ARTICLE_0028</td>\n      <td>Google I/O 2016: Android's failure to innovate...</td>\n      <td>Rather than focusing on the incremental innova...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0222</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>ARTICLE_0029</td>\n      <td>Google+ is five years old today</td>\n      <td>Somehow, don't ask me how , Google's zombie of...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1179</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>ARTICLE_0030</td>\n      <td>Large CPGs are under attack by startups... and...</td>\n      <td>Consumer packaged goods are big business - val...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1420</td>\n      <td>BR</td>\n      <td>SP</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>ARTICLE_0031</td>\n      <td>New Asia Trend Briefing from TrendWatching | S...</td>\n      <td>The online world has allowed for faster, easie...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_1063</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>ARTICLE_0032</td>\n      <td>Honda fortalece estratégia para desenvolver te...</td>\n      <td>De acordo com a empresa, o local, previsto par...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_0603</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>ARTICLE_0033</td>\n      <td>Java 8 Streams - A Deeper Approach About Perfo...</td>\n      <td>Introduction Java 8 was released almost three ...</td>\n      <td>HTML</td>\n      <td>en</td>\n      <td>USER_0657</td>\n      <td>BR</td>\n      <td>SP</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>ARTICLE_0034</td>\n      <td>E quando o cliente diz \"Não gostei\"?</td>\n      <td>Dez entre dez webdesigners se tremem ao ouvir ...</td>\n      <td>HTML</td>\n      <td>pt</td>\n      <td>USER_1183</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_info.head(35)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:49:41.683359800Z",
     "start_time": "2024-06-06T09:49:41.655433200Z"
    }
   },
   "id": "8ebd850688b9aab7"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# 사전학습된 모델과 토크나이저 로드\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:47:58.686058100Z",
     "start_time": "2024-06-06T09:47:57.681991200Z"
    }
   },
   "id": "100b9fd720f812e5"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# # 모델과 토크나이저 로드\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:47:27.045153700Z",
     "start_time": "2024-06-06T09:47:24.799606100Z"
    }
   },
   "id": "3ac9d7cab2007a01"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n  )\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU 사용 여부 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:48:01.054853800Z",
     "start_time": "2024-06-06T09:48:00.688919400Z"
    }
   },
   "id": "dbad6ea7cde0d8ef"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 예측 함수 정의\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:48:03.209305500Z",
     "start_time": "2024-06-06T09:48:03.202324300Z"
    }
   },
   "id": "2b7d9538588334c5"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m article_info[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mContent\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m----> 4\u001B[0m     label \u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(label)\n",
      "Cell \u001B[1;32mIn[26], line 3\u001B[0m, in \u001B[0;36mpredict\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(text):\n\u001B[1;32m----> 3\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2829\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[0;32m   2828\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[1;32m-> 2829\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_one(text\u001B[38;5;241m=\u001B[39mtext, text_pair\u001B[38;5;241m=\u001B[39mtext_pair, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mall_kwargs)\n\u001B[0;32m   2830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2831\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2935\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2915\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[0;32m   2916\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[0;32m   2917\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2932\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2933\u001B[0m     )\n\u001B[0;32m   2934\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2935\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[0;32m   2936\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[0;32m   2937\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   2938\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   2939\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[0;32m   2940\u001B[0m         truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   2941\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m   2942\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m   2943\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[0;32m   2944\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m   2945\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m   2946\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m   2947\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m   2948\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m   2949\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m   2950\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m   2951\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m   2952\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m   2953\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2954\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3008\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode_plus\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2998\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[0;32m   2999\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[0;32m   3000\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[0;32m   3001\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3005\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3006\u001B[0m )\n\u001B[1;32m-> 3008\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encode_plus(\n\u001B[0;32m   3009\u001B[0m     text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[0;32m   3010\u001B[0m     text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   3011\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   3012\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[0;32m   3013\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[0;32m   3014\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[0;32m   3015\u001B[0m     stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[0;32m   3016\u001B[0m     is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[0;32m   3017\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[0;32m   3018\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[0;32m   3019\u001B[0m     return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[0;32m   3020\u001B[0m     return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[0;32m   3021\u001B[0m     return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[0;32m   3022\u001B[0m     return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[0;32m   3023\u001B[0m     return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[0;32m   3024\u001B[0m     return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[0;32m   3025\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m   3026\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3027\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\tokenization_utils.py:719\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._encode_plus\u001B[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_offsets_mapping:\n\u001B[0;32m    711\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[0;32m    712\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    713\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    716\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    717\u001B[0m     )\n\u001B[1;32m--> 719\u001B[0m first_ids \u001B[38;5;241m=\u001B[39m \u001B[43mget_input_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    720\u001B[0m second_ids \u001B[38;5;241m=\u001B[39m get_input_ids(text_pair) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    722\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_for_model(\n\u001B[0;32m    723\u001B[0m     first_ids,\n\u001B[0;32m    724\u001B[0m     pair_ids\u001B[38;5;241m=\u001B[39msecond_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    738\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m    739\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\tokenization_utils.py:686\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m    684\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_input_ids\u001B[39m(text):\n\u001B[0;32m    685\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 686\u001B[0m         tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenize(text, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    687\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(tokens)\n\u001B[0;32m    688\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\tokenization_utils.py:617\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.tokenize\u001B[1;34m(self, text, **kwargs)\u001B[0m\n\u001B[0;32m    615\u001B[0m         tokenized_text\u001B[38;5;241m.\u001B[39mappend(token)\n\u001B[0;32m    616\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 617\u001B[0m         tokenized_text\u001B[38;5;241m.\u001B[39mextend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    618\u001B[0m \u001B[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001B[39;00m\n\u001B[0;32m    619\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_text\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta.py:304\u001B[0m, in \u001B[0;36mRobertaTokenizer._tokenize\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Tokenize a string.\"\"\"\u001B[39;00m\n\u001B[0;32m    303\u001B[0m bpe_tokens \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m--> 304\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfindall\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    305\u001B[0m     token \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbyte_encoder[b] \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m token\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    307\u001B[0m     )  \u001B[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001B[39;00m\n\u001B[0;32m    308\u001B[0m     bpe_tokens\u001B[38;5;241m.\u001B[39mextend(bpe_token \u001B[38;5;28;01mfor\u001B[39;00m bpe_token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbpe(token)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\study\\lib\\site-packages\\regex\\regex.py:338\u001B[0m, in \u001B[0;36mfindall\u001B[1;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, timeout, ignore_unused, **kwargs)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return a list of all matches in the string. The matches may be overlapped\u001B[39;00m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;124;03mif overlapped is True. If one or more groups are present in the pattern,\u001B[39;00m\n\u001B[0;32m    335\u001B[0m \u001B[38;5;124;03mreturn a list of groups; this will be a list of tuples if the pattern has\u001B[39;00m\n\u001B[0;32m    336\u001B[0m \u001B[38;5;124;03mmore than one group. Empty matches are included in the result.\"\"\"\u001B[39;00m\n\u001B[0;32m    337\u001B[0m pat \u001B[38;5;241m=\u001B[39m _compile(pattern, flags, ignore_unused, kwargs, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 338\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfindall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstring\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendpos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverlapped\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconcurrent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "\n",
    "for text in article_info['Content']:\n",
    "    label = predict(text)\n",
    "    print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T09:50:53.530268100Z",
     "start_time": "2024-06-06T09:50:46.465234900Z"
    }
   },
   "id": "dbeca52331153f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3fa76d66a23e0e63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
